# ğŸ§  FedGA-Meta: Genetic and Meta-Learning Aggregation for Federated Learning in Industrial Cyber-Physical Systems

## ğŸ“˜ Overview
**FedGA-Meta** is a hierarchical federated learning (FL) framework designed for **Industrial Cyber-Physical Systems (ICPS)**.  
It tackles key FL challenges such as **data heterogeneity**, **domain shift**, **limited communication resources**, and **participant variability**.  

The framework integrates:
- **Genetic Algorithms (FedGA)** for adaptive and efficient model aggregation,  
- **Model-Agnostic Meta-Learning (MAML)** to enhance generalization to new participants,  
- **CORAL (CORrelation ALignment)** for gradient alignment and domain adaptation,  
- And a **hierarchical architecture** (Edge â†’ Fog â†’ Cloud) that optimizes computation and communication.

## âš™ï¸ Architecture
FedGA-Meta relies on a three-layer hierarchy:

### ğŸ§© Edge Layer
- Local collaborators (robots, sensors, IoT devices, etc.) perform local training on private data.  
- Each edge model is periodically sent to its assigned **fog server**.

### â˜ï¸ Fog Layer
- Each fog server aggregates local edge models using an **enhanced Genetic Algorithm (FedGA)**.  
- Applies the **inner-update** phase of MAML on its benchmark subset.  
- Sends **gradients of base layers** (not full models) to the cloud to reduce communication cost.

### ğŸŒ Cloud Layer
- Aligns gradients across fog domains using **CORAL alignment**.  
- Applies the **outer-update** phase of MAML for global adaptation.  
- Broadcasts the updated global base model back to all fog servers.

## ğŸ§¬ Workflow Summary
1. Initialize the global model at the cloud and distribute to fog servers.  
2. Each fog integrates the global base model with its local extractor.  
3. Edge collaborators train locally for several epochs.  
4. Fog servers aggregate local models via **Enhanced FedGA**.  
5. Fog servers perform **MAML inner-update** and send gradients.  
6. Cloud performs **CORAL alignment** and **MAML outer-update**.  
7. Updated global weights are redistributed to all fogs.  

## ğŸ§  Algorithms
FedGA-Meta includes the following routines:

- **EdgeUpdate** â€” local learning at each participant  
- **FogAggregation** â€” partial aggregation using FedGA + MAML inner-update  
- **CloudAggregation** â€” global aggregation using CORAL + MAML outer-update  
- **FedGA-Meta (Main Routine)** â€” orchestrates the full FL workflow  

Each step is formally defined in the implementation and algorithms (see Algorithms 2â€“5 in the manuscript).

## ğŸ§ª Experiments
The framework was evaluated on five benchmark datasets to simulate **domain-shifted** FL environments:

| Fog | Dataset  | Feature Space | Type  |
|-----|-----------|---------------|-------|
| f1  | MNIST     | 28Ã—28Ã—1       | Grayscale |
| f2  | USPS      | 16Ã—16Ã—1       | Grayscale |
| f3  | SVHN      | 32Ã—32Ã—3       | RGB |
| f4  | EMNIST    | 28Ã—28Ã—1       | Grayscale |
| f5  | MNIST-M   | 28Ã—28Ã—3       | RGB |

- Each fog manages **10 edge participants** (50 total).  
- Datasets were partitioned with a **Dirichlet Î± = 0.5** to simulate **Non-IID** distributions.  
- Training was conducted over **50 FL rounds** using a workstation with:
  - NVIDIA RTX 3090 GPU  
  - Intel Core i9 CPU  
  - 32 GB RAM  

### ğŸ” Evaluation Metrics
- Test Loss (cross-entropy)  
- Accuracy  
- F1-Score  
- Expected Calibration Error (ECE)  
- Worst 10 % Accuracy (Fairness indicator)

### ğŸ“Š Comparative Frameworks
FedGA-Meta was compared to:
- FedAvg  
- FedPer  
- FedProx  
- FedMAML  
- FedGA  

### ğŸš€ Results
FedGA-Meta outperforms all baselines in:
- **Adaptability** â€” higher accuracy & F1 under domain shift.  
- **Generalization** â€” smooth adaptation to late-joining participants.  
- **Cost-effectiveness** â€” better trade-off between local computation and communication.  


## ğŸ§± Technologies & Dependencies
FedGA-Meta is implemented in **Python 3.9** and leverages several scientific and machine learning libraries to ensure modularity, flexibility, and scalability of experiments.

### Core Libraries
- **torch** â€” Deep learning framework for model definition, training, and gradient computation.  
- **pygad** â€” Genetic algorithm optimization for partial aggregation at the fog layer.  
- **tsne-torch** â€” Visualization of high-dimensional model embeddings and feature distributions.  
- **numpy** â€” Numerical computation and tensor manipulation.  
- **pandas** â€” Dataset management, preprocessing, and statistical analysis.  
- **scikit-learn** â€” Data splitting, normalization, and evaluation metrics.  
- **scipy** â€” Scientific computations and numerical optimization utilities.  
- **seaborn** â€” Statistical visualization of metrics and distributions.  
- **matplotlib** â€” Plotting and result visualization for training curves and heatmaps.
  

### Installation
```bash
pip install -r requirements.txt
```

## ğŸš€ Usage


### ğŸ”¹ Run all experiments automatically
To execute all comparative frameworks (FedAvg, FedProx, FedPer, FedMAML, FedGA, and FedGA-Meta) under different local training configurations, simply run:


#### On Windows:
```bash
lunch-project.bat
```

#### On MacOS:
```bash
chmod +x run_all.sh
./run_all.sh
```
### ğŸ”¹ Visualize results

After the training phase is completed, all results (metrics, plots, and LaTeX tables) are automatically saved inside:
```bash
results_algos/
```

To visualize the metrics as figures (e.g., Accuracy, F1-score, ECE, and fairness), run:
```bash
python visualise_metrics.py

```

To display or export the summary tables in LaTeX or terminal format, run:
```bash
python visualise_tabs.py

```
